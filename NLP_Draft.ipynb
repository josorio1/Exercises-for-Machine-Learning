{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Exercises\n",
    "\n",
    "We have five exercises in this section. The exercises are:\n",
    "1. Build your own tokenizer, where you need to implement two functions to implement a tokenizer based on regular expression.\n",
    "2. Get tags from Trump speech.\n",
    "3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
    "4. Build your own Bag Of Words implementation using tokenizer created before.\n",
    "5. Build a 5-gram model and clean up the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1. Build your own tokenizer\n",
    "\n",
    "Build two different tokenizers:\n",
    "- ``tokenize_sentence``: function tokenizing text into sentences,\n",
    "- ``tokenize_word``: function tokenizing text into words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://note.nkmk.me/en/python-split-rsplit-splitlines-re/#:~:text=If%20you%20want%20to%20split,()%20of%20the%20re%20module.&text=In%20re.,consecutive%20numbers%20is%20as%20follows.\n",
    "\n",
    "https://regexusingpython.wordpress.com/2019/04/02/regex-module-lets-begin/\n",
    "\n",
    "https://www.geeksforgeeks.org/regular-expression-python-examples-set-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences:\n",
      "['Here we go again', ' I was supposed to add this text later', \"Well, it's 10\", 'p', 'm', \" here, and I'm actually having fun making this course\", ' :oI hope you are getting along fine with this presentation, I really did try', 'And one last sentence, just so you can test you tokenizers better']\n",
      "Tokenized words:\n",
      "['Here', 'we', 'go', 'again', 'I', 'was', 'supposed', 'to', 'add', 'this', 'text', 'later', 'Well', 'it', 's', '10', 'p', 'm', 'here', 'and', 'I', 'm', 'actually', 'having', 'fun', 'making', 'this', 'course', 'oI', 'hope', 'you', 'are', 'getting', 'along', 'fine', 'with', 'this', 'presentation', 'I', 'really', 'did', 'try', 'And', 'one', 'last', 'sentence', 'just', 'so', 'you', 'can', 'test', 'you', 'tokenizers', 'better']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize_words(text: str) -> list:\n",
    "    pattern = \"\\w+\" #this works pretty well, although it is not perfect.\n",
    "    words = re.findall(pattern, text)\n",
    "\n",
    "\n",
    "    return words\n",
    "\n",
    "def tokenize_sentence(text: str) -> list:\n",
    "    \n",
    "    pattern = '[^.=\\?\\!]+.=\\?\\!|[^.=\\?\\!]+'\n",
    "    sentences = re.findall(pattern, text)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "text = \"Here we go again. I was supposed to add this text later.\\\n",
    "Well, it's 10.p.m. here, and I'm actually having fun making this course. :o\\\n",
    "I hope you are getting along fine with this presentation, I really did try.\\\n",
    "And one last sentence, just so you can test you tokenizers better.\"\n",
    "text2= \"Sos puto?Yo= creo. que si\"\n",
    "print(\"Tokenized sentences:\")\n",
    "print(tokenize_sentence(text))\n",
    "\n",
    "print(\"Tokenized words:\")\n",
    "print(tokenize_words(text))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The tokenizer is not perfect, but I think it works fine. I cannot get to separate:oI properly.\n",
    " \n",
    " As you can see, I also have problems with the p.m if I am only allowed to use regex. Otherwise, I would go with an approach similar to my colleague Cibr√°n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. Get tags from Trump speech using NLTK\n",
    "\n",
    "You should use the ``trump.txt`` file, read it and find the tags for each word. Use NLTK for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to download a couple of things for it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jackt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jackt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the file and use pos_tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "file = open(\"./datasets/trump.txt\", \"r\",encoding=\"utf-8\") \n",
    "trump = file.read()\n",
    "words = word_tokenize(trump)\n",
    "tags = pos_tag(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we read everything nicely in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>very</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>much</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Speaker</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vice</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>President</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Members</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Congress</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>First</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lady</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Words Tags\n",
       "0       Thank  NNP\n",
       "1         you  PRP\n",
       "2        very   RB\n",
       "3        much   RB\n",
       "4           .    .\n",
       "5         Mr.  NNP\n",
       "6     Speaker  NNP\n",
       "7           ,    ,\n",
       "8         Mr.  NNP\n",
       "9        Vice  NNP\n",
       "10  President  NNP\n",
       "11          ,    ,\n",
       "12    Members  NNP\n",
       "13         of   IN\n",
       "14   Congress  NNP\n",
       "15          ,    ,\n",
       "16        the   DT\n",
       "17      First  NNP\n",
       "18       Lady  NNP\n",
       "19         of   IN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame( {\"Words\": [item[0] for item in tags], \"Tags\": [item[1] for item in tags]})\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to find the tag for a given word we can use the following function. We will try with 'Thank', which can be a verb or a noun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_word(word):\n",
    "    return print(word,\"can be tagged as\",set(df[\"Tags\"][df[\"Words\"] == word])) \n",
    "    #We use set() to return unique tags\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank can be tagged as {'VB', 'NNP'}\n"
     ]
    }
   ],
   "source": [
    "tag_word(\"Thank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
    "\n",
    "Please use Python list features to get the last 10 sentences and display nouns from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 288 : When we fulfill this vision, when we celebrate our 250 years of glorious freedom, we will look back on tonight as when this new chapter of American greatness began.\n",
      "Nouns in sentence 288 : [vision, years, freedom, tonight, chapter, greatness]\n",
      "Sentence 289 : The time for small thinking is over.\n",
      "Nouns in sentence 289 : [time, thinking]\n",
      "Sentence 290 : The time for trivial fights is behind us.\n",
      "Nouns in sentence 290 : [time, fights]\n",
      "Sentence 291 : We just need the courage to share the dreams that fill our hearts, the bravery to express the hopes that stir our souls, and the confidence to turn those hopes and those dreams into action.\n",
      "\n",
      "\n",
      "Nouns in sentence 291 : [courage, dreams, hearts, bravery, hopes, souls, confidence, hopes, dreams, action]\n",
      "Sentence 292 : From now on, America will be empowered by our aspirations, not burdened by our fears; inspired by the future, not bound by failures of the past; and guided by our vision, not blinded by our doubts.\n",
      "\n",
      "\n",
      "Nouns in sentence 292 : [aspirations, fears, future, failures, past, vision, doubts]\n",
      "Sentence 293 : I am asking all citizens to embrace this renewal of the American spirit.\n",
      "Nouns in sentence 293 : [citizens, renewal]\n",
      "Sentence 294 : I am asking all Members of Congress to join me in dreaming big and bold, and daring things for our country.\n",
      "Nouns in sentence 294 : [things, country]\n",
      "Sentence 295 : I am asking everyone watching tonight to seize this moment.\n",
      "Nouns in sentence 295 : [tonight, moment]\n",
      "Sentence 296 : Believe in yourselves, believe in your future, and believe, once more, in America.\n",
      "\n",
      "\n",
      "Nouns in sentence 296 : [yourselves, future]\n",
      "Sentence 297 : Thank you, God bless you, and God bless the United States.\n",
      "Nouns in sentence 297 : []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "file = open(\"./datasets/trump.txt\", \"r\",encoding='utf-8') \n",
    "trump = file.read() \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(trump)\n",
    "\n",
    "\n",
    "n_sentences = 0 \n",
    "for span in doc.sents: #We have to know the number of sentences.\n",
    "       n_sentences += 1\n",
    "\n",
    "n_aux = 0\n",
    "\n",
    "for span in doc.sents: # I think these lines look horrible, but I didn't find anything better on the Internet. \n",
    "    n_aux += 1\n",
    "    if n_aux >(n_sentences-10):\n",
    "        print(\"Sentence\",n_aux,\":\",span)\n",
    "        nouns_sentence=[]\n",
    "        for i in range(span.start, span.end):\n",
    "         token = doc[i]\n",
    "         if token.pos_ == 'NOUN':\n",
    "          nouns_sentence.append(token)\n",
    "        print(\"Nouns in sentence\",n_aux,\":\", nouns_sentence)\n",
    "        \n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I understand that God and United States, for example, do not count as nouns (https://spacy.io/usage/linguistic-features#pos-tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. Build your own Bag Of Words implementation using tokenizer created before \n",
    "\n",
    "You need to implement following methods:\n",
    "\n",
    "- ``fit_transform`` - gets a list of strings and returns matrix with it's BoW representation\n",
    "- ``get_features_names`` - returns list of words corresponding to columns in BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "class BagOfWords:\n",
    "    \"\"\"Basic BoW implementation.\"\"\"\n",
    "    \n",
    "    __nlp = spacy.load(\"en_core_web_sm\")\n",
    "    __bow_list = []\n",
    "    \n",
    "    # your code goes maybe also here    \n",
    "    \n",
    "    def fit_transform(self, corpus: list):\n",
    "        \n",
    "        #We create the dictionary and save the frequency:\n",
    "        dictionary = []\n",
    "        frequency = []\n",
    "        words_sentence = []\n",
    "        for sentence in corpus:\n",
    "         words = tokenize_words(sentence)\n",
    "         words_sentence.append(words)\n",
    "         for word in words:\n",
    "            if word not in dictionary:\n",
    "                dictionary.append(word)\n",
    "                frequency.append(0)\n",
    "            else:\n",
    "                frequency[dictionary.index(word)] += 1\n",
    "                \n",
    "         #We sort the dictionary according to the frequency:\n",
    "        \n",
    "        frequency_arg_sort = np.argsort(frequency) #sorted from less frequent to most frequent\n",
    "        dictionary_sort = []\n",
    "        for i in range(len(dictionary)):\n",
    "            dictionary_sort.append(dictionary[frequency_arg_sort[-1-i]])\n",
    "            \n",
    "        # We check how many times the words in our dictionary appear in each sentence:\n",
    "        \n",
    "        bow_list=[]\n",
    "        \n",
    "        for i in range(len(words_sentence)):\n",
    "            bow_list.append([0]*len(dictionary_sort))\n",
    "            for j in range(len(words_sentence[i])):\n",
    "                for k in range(len(dictionary_sort)):\n",
    "                    if dictionary_sort[k] == words_sentence[i][j]:\n",
    "                        bow_list[i][k] += 1\n",
    "        \n",
    "        self.dictionary_sort = dictionary_sort\n",
    "        \n",
    "        return np.array([bow_list])\n",
    "    \n",
    "    def get_feature_names(self) -> list:\n",
    "        \n",
    "        return self.dictionary_sort        \n",
    "        \n",
    "corpus = [\n",
    "    'Bag Of Words is based on counting',\n",
    "     'words occurences throughout multiple documents.',\n",
    "     'This is the third document.',\n",
    "     'As you can see most of the words occur only once.',\n",
    "     'This gives us a pretty sparse matrix, see below. Really, see below']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "BagOfWords = BagOfWords()\n",
    "\n",
    "bow= BagOfWords.fit_transform(corpus)\n",
    "dictionary = BagOfWords.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "  [0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0]\n",
      "  [2 0 0 1 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0]]]\n",
      "['see', 'words', 'the', 'This', 'below', 'is', 'based', 'on', 'counting', 'third', 'occurences', 'throughout', 'multiple', 'documents', 'Words', 'Of', 'Really', 'As', 'document', 'you', 'can', 'most', 'of', 'occur', 'only', 'once', 'gives', 'us', 'a', 'pretty', 'sparse', 'matrix', 'Bag']\n"
     ]
    }
   ],
   "source": [
    "print(bow)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5. Build a 5-gram model and clean up the results.\n",
    "\n",
    "There are three tasks to do:\n",
    "1. Use 5-gram model instead of 3.\n",
    "2. Change to capital letter each first letter of a sentence.\n",
    "3. Remove the whitespace between the last word in a sentence and . ! or ?.\n",
    "\n",
    "Hint: for 2. and 3. implement a function called ``clean_generated()`` that takes the generated text and fix both issues at once. It could be easier to fix the text after it's generated rather then doing some changes in the while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "\n",
    "wall_street = text7.tokens\n",
    "\n",
    "import re\n",
    "\n",
    "tokens = wall_street\n",
    "\n",
    "def cleanup():\n",
    "    compiled_pattern = re.compile(\"^[a-zA-Z0-9.!?]\")\n",
    "    clean = list(filter(compiled_pattern.match,tokens))\n",
    "    return clean\n",
    "tokens = cleanup()\n",
    "\n",
    "def build_ngrams():\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-N+1):\n",
    "        ngrams.append(tokens[i:i+N])\n",
    "    return ngrams\n",
    "\n",
    "def ngram_freqs(ngrams):\n",
    "    counts = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        token_seq  = SEP.join(ngram[:-1])\n",
    "        last_token = ngram[-1]\n",
    "\n",
    "        if token_seq not in counts:\n",
    "            counts[token_seq] = {}\n",
    "\n",
    "        if last_token not in counts[token_seq]:\n",
    "            counts[token_seq][last_token] = 0\n",
    "\n",
    "        counts[token_seq][last_token] += 1;\n",
    "\n",
    "    return counts\n",
    "\n",
    "def next_word(text, N, counts):\n",
    "\n",
    "    token_seq = SEP.join(text.split()[-(N-1):]);\n",
    "    choices = counts[token_seq].items();\n",
    "\n",
    "    total = sum(weight for choice, weight in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for choice, weight in choices:\n",
    "        upto += weight;\n",
    "        if upto > r: return choice\n",
    "    assert False # should not reach here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_generated():\n",
    "    # put your code here\n",
    "    pass\n",
    "\n",
    "N=3 # fix it for other value of N\n",
    "\n",
    "SEP=\" \"\n",
    "\n",
    "sentence_count=5\n",
    "\n",
    "ngrams = build_ngrams()\n",
    "start_seq=\"We have\"\n",
    "\n",
    "counts = ngram_freqs(ngrams)\n",
    "\n",
    "if start_seq is None: start_seq = random.choice(list(counts.keys()))\n",
    "generated = start_seq.lower();\n",
    "\n",
    "sentences = 0\n",
    "while sentences < sentence_count:\n",
    "    generated += SEP + next_word(generated, N, counts)\n",
    "    sentences += 1 if generated.endswith(('.','!', '?')) else 0\n",
    "\n",
    "# put your code here:\n",
    "\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
